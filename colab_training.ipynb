{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üß¨ Geometry-Complete Equivariant Diffusion\n",
        "## De Novo Drug Design Training\n",
        "\n",
        "**Cluster-Based Split**: 100 pockets √ó 50 ligands = 5,000 pairs\n",
        "\n",
        "**No Data Leakage**: Same pocket never in both train and val"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": ["## Cell 1: Setup"],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import torch\n",
        "print(f'GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"None!\"}')\n",
        "\n",
        "!pip install -q torch-geometric rdkit scipy numpy pyyaml tqdm gdown\n",
        "print('‚úÖ Setup complete')"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": ["## Cell 2: Clone Repository"],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "REPO = '/content/drive/MyDrive/geom_diffusion'\n",
        "if not os.path.exists(REPO):\n",
        "    !git clone https://github.com/Nethrananda21/geom_diffusion.git {REPO}\n",
        "%cd {REPO}\n",
        "!git pull origin master"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 3: Download Pre-processed Data\n",
        "\n",
        "Downloads DiffSBDD's CrossDocked data (~500MB)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gdown\n",
        "\n",
        "DATA_DIR = '/content/data/crossdocked'\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "\n",
        "# DiffSBDD pre-processed files\n",
        "files = {\n",
        "    'train_data.pkl': '1vJyxCIqCYwP3qj4THMofdSd1rZDEQpPG',\n",
        "    'val_data.pkl': '1FpVNcdj0R5YOsaLQm6T4D5QOKZGI4Xc5'\n",
        "}\n",
        "\n",
        "for fname, fid in files.items():\n",
        "    path = f'{DATA_DIR}/{fname}'\n",
        "    if not os.path.exists(path):\n",
        "        print(f'üì• Downloading {fname}...')\n",
        "        gdown.download(id=fid, output=path, quiet=False)\n",
        "    else:\n",
        "        print(f'‚úÖ {fname} exists')\n",
        "\n",
        "!ls -la {DATA_DIR}"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 4: Create Cluster-Based 5K Subset\n",
        "\n",
        "**Strategy**: 100 pockets √ó 50 ligands = 5,000 pairs\n",
        "\n",
        "**No Leakage**: Pockets are disjoint between train (80) and val (20)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import random\n",
        "from collections import defaultdict\n",
        "\n",
        "random.seed(42)\n",
        "\n",
        "# Load full data\n",
        "with open(f'{DATA_DIR}/train_data.pkl', 'rb') as f:\n",
        "    full_data = pickle.load(f)\n",
        "print(f'Loaded {len(full_data)} samples')\n",
        "\n",
        "# Group by pocket\n",
        "pockets = defaultdict(list)\n",
        "for sample in full_data:\n",
        "    # Extract pocket ID from sample\n",
        "    pocket_id = sample.get('pocket_id', sample.get('receptor', str(hash(str(sample['pocket_coords'][:5])))))\n",
        "    pockets[pocket_id].append(sample)\n",
        "\n",
        "print(f'Found {len(pockets)} unique pockets')\n",
        "\n",
        "# Select 120 pockets (100 train + 20 val)\n",
        "pocket_ids = list(pockets.keys())\n",
        "random.shuffle(pocket_ids)\n",
        "selected_pockets = pocket_ids[:120]\n",
        "\n",
        "# Split: 100 train, 20 val (NO OVERLAP!)\n",
        "train_pockets = selected_pockets[:100]\n",
        "val_pockets = selected_pockets[100:120]\n",
        "\n",
        "print(f'Train pockets: {len(train_pockets)}')\n",
        "print(f'Val pockets: {len(val_pockets)}')\n",
        "print(f'Overlap: {len(set(train_pockets) & set(val_pockets))} (should be 0!)')\n",
        "\n",
        "# Select 50 ligands per train pocket\n",
        "train_samples = []\n",
        "for pid in train_pockets:\n",
        "    ligands = pockets[pid]\n",
        "    selected = ligands[:50] if len(ligands) >= 50 else ligands\n",
        "    train_samples.extend(selected)\n",
        "\n",
        "# Select 50 ligands per val pocket\n",
        "val_samples = []\n",
        "for pid in val_pockets:\n",
        "    ligands = pockets[pid]\n",
        "    selected = ligands[:50] if len(ligands) >= 50 else ligands\n",
        "    val_samples.extend(selected)\n",
        "\n",
        "print(f'\\n‚úÖ Cluster-Based Split:')\n",
        "print(f'   Train: {len(train_samples)} samples ({len(train_pockets)} pockets)')\n",
        "print(f'   Val: {len(val_samples)} samples ({len(val_pockets)} pockets)')\n",
        "\n",
        "# Save subset\n",
        "with open(f'{DATA_DIR}/train_5k.pkl', 'wb') as f:\n",
        "    pickle.dump(train_samples, f)\n",
        "with open(f'{DATA_DIR}/val_1k.pkl', 'wb') as f:\n",
        "    pickle.dump(val_samples, f)\n",
        "\n",
        "print(f'\\nüíæ Saved: train_5k.pkl, val_1k.pkl')"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": ["## Cell 5: Update Config to Use Subset"],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import yaml\n",
        "\n",
        "%cd /content/drive/MyDrive/geom_diffusion\n",
        "\n",
        "with open('configs/debug_t4.yaml', 'r') as f:\n",
        "    cfg = yaml.safe_load(f)\n",
        "\n",
        "# Point to local data with our subset\n",
        "cfg['data']['root'] = '/content/data'\n",
        "cfg['data']['train_file'] = 'crossdocked/train_5k.pkl'\n",
        "cfg['data']['val_file'] = 'crossdocked/val_1k.pkl'\n",
        "\n",
        "# Training settings\n",
        "cfg['training']['max_epochs'] = 50\n",
        "cfg['hardware']['num_workers'] = 2\n",
        "\n",
        "with open('configs/debug_t4.yaml', 'w') as f:\n",
        "    yaml.dump(cfg, f)\n",
        "\n",
        "print('‚úÖ Config updated to use cluster-based 5K subset')"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": ["## Cell 6: Delete Cache"],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "cache = Path('/content/data/cache')\n",
        "if cache.exists():\n",
        "    shutil.rmtree(cache)\n",
        "    print('üóëÔ∏è Cache deleted')\n",
        "else:\n",
        "    print('‚ÑπÔ∏è No cache')"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": ["## Cell 7: Train üöÄ"],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/geom_diffusion\n",
        "!python train.py --config configs/debug_t4.yaml --checkpoint_dir checkpoints"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": ["## Cell 8: Resume (If Disconnected)"],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Run Cells 1, 2, 5, 6 first, then:\n",
        "# %cd /content/drive/MyDrive/geom_diffusion\n",
        "# !python train.py --config configs/debug_t4.yaml --resume checkpoints/best_model.pt"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}
