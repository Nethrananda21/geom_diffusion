{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ§¬ Geometry-Complete Equivariant Diffusion Model\n",
        "## De Novo Drug Design Training Notebook\n",
        "\n",
        "**Requirements:** GPU Runtime (T4), ~55GB disk space"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": ["## Cell 1: Check GPU & Disk Space"],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import shutil\n",
        "\n",
        "# GPU check\n",
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"âœ… GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"   VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "else:\n",
        "    print(\"âŒ No GPU! Runtime > Change runtime type > GPU\")\n",
        "\n",
        "# Disk check\n",
        "total, used, free = shutil.disk_usage('/')\n",
        "print(f\"\\nðŸ’¾ Disk: {free / 1e9:.1f} GB free of {total / 1e9:.1f} GB\")\n",
        "if free / 1e9 < 55:\n",
        "    print(\"âš ï¸ Less than 55GB free - may not fit full dataset\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": ["## Cell 2: Install Dependencies"],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q torch-geometric rdkit scipy numpy pyyaml tqdm wandb\n",
        "from rdkit import Chem\n",
        "print(\"âœ… All dependencies installed\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": ["## Cell 3: Clone Repository"],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Nethrananda21/geom_diffusion.git\n",
        "%cd geom_diffusion\n",
        "!git pull origin master"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 4: Download & Extract Dataset (Stream Mode - Saves 50GB!)\n",
        "\n",
        "âš ï¸ This downloads and extracts simultaneously, never storing the full 50GB archive.\n",
        "\n",
        "**Skip this cell to use synthetic data for quick testing.**"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Create data directory\n",
        "os.makedirs('data', exist_ok=True)\n",
        "\n",
        "print(\"ðŸ“¥ Downloading and extracting CrossDocked2020 (streaming mode)...\")\n",
        "print(\"   This takes 30-60 minutes. Progress bar shows download status.\")\n",
        "print(\"   Do NOT close the browser!\\n\")\n",
        "\n",
        "# Stream download + extract WITH PROGRESS BAR (using curl)\n",
        "!curl -L --progress-bar http://bits.csb.pitt.edu/files/crossdock2020/CrossDocked2020_v1.3.tgz | tar -xzf - -C ./data/\n",
        "\n",
        "# Verify extraction\n",
        "if os.path.exists('data/CrossDocked2020'):\n",
        "    print(\"\\nâœ… Dataset extracted successfully to data/CrossDocked2020/\")\n",
        "    !du -sh data/CrossDocked2020\n",
        "else:\n",
        "    # Check if extracted with different folder name\n",
        "    !ls -la data/\n",
        "    print(\"\\nâš ï¸ Check folder name above - may need to rename\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": ["## Cell 5: Preprocess Dataset (Creates .pkl files)"],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the extracted folder (might have different name)\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "data_dir = None\n",
        "for folder in ['CrossDocked2020', 'crossdocked2020', 'CrossDocked2020_v1.3']:\n",
        "    if Path(f'data/{folder}').exists():\n",
        "        data_dir = f'data/{folder}'\n",
        "        break\n",
        "\n",
        "if data_dir:\n",
        "    print(f\"ðŸ“‚ Found dataset at: {data_dir}\")\n",
        "    print(\"â³ Preprocessing (this takes 10-20 minutes)...\")\n",
        "    !python preprocess_crossdocked.py \\\n",
        "        --data_dir {data_dir} \\\n",
        "        --output_dir ./data/crossdocked \\\n",
        "        --config configs/debug_t4.yaml\n",
        "    print(\"\\nâœ… Preprocessing complete\")\n",
        "    !ls -la data/crossdocked/\n",
        "else:\n",
        "    print(\"âŒ Dataset folder not found. Check Cell 4 output.\")\n",
        "    !ls -la data/"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": ["## Cell 6: Delete Old Cache (CRITICAL!)"],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "# Delete synthetic cache\n",
        "cache_dir = Path('data/cache')\n",
        "if cache_dir.exists():\n",
        "    shutil.rmtree(cache_dir)\n",
        "    print(\"ðŸ—‘ï¸ Deleted old cache\")\n",
        "else:\n",
        "    print(\"â„¹ï¸ No cache to delete\")\n",
        "\n",
        "# Verify real data\n",
        "train_pkl = Path('data/crossdocked/train_data.pkl')\n",
        "val_pkl = Path('data/crossdocked/val_data.pkl')\n",
        "\n",
        "if train_pkl.exists() and val_pkl.exists():\n",
        "    print(f\"\\nâœ… Ready to train on REAL data:\")\n",
        "    print(f\"   - {train_pkl} ({train_pkl.stat().st_size / 1e6:.1f} MB)\")\n",
        "    print(f\"   - {val_pkl} ({val_pkl.stat().st_size / 1e6:.1f} MB)\")\n",
        "else:\n",
        "    print(\"\\nâš ï¸ Real data NOT found - will use SYNTHETIC\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": ["## Cell 7: Configure Training"],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import yaml\n",
        "\n",
        "with open('configs/debug_t4.yaml', 'r') as f:\n",
        "    config = yaml.safe_load(f)\n",
        "\n",
        "# Colab optimizations\n",
        "config['training']['max_epochs'] = 10  # Quick test\n",
        "config['hardware']['num_workers'] = 2  # Colab CPU limit\n",
        "\n",
        "with open('configs/debug_t4.yaml', 'w') as f:\n",
        "    yaml.dump(config, f)\n",
        "\n",
        "print(f\"âœ… Config: batch={config['training']['batch_size']}, epochs={config['training']['max_epochs']}\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": ["## Cell 8: Start Training ðŸš€"],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Watch logs - should say \"Loading preprocessed data\" NOT \"SYNTHETIC\"\n",
        "!python train.py --config configs/debug_t4.yaml --checkpoint_dir ./checkpoints"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": ["## Cell 9: Resume Training (If Interrupted)"],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Uncomment to resume\n",
        "# !python train.py --config configs/debug_t4.yaml --resume ./checkpoints/best_model.pt"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": ["## Cell 10: Download Trained Model"],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "from pathlib import Path\n",
        "\n",
        "ckpt = Path('checkpoints/best_model.pt')\n",
        "if ckpt.exists():\n",
        "    files.download(str(ckpt))\n",
        "    print(\"âœ… Model downloaded\")\n",
        "else:\n",
        "    print(\"âŒ No checkpoint yet\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}
