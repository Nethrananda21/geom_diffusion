{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ§¬ Geometry-Complete Equivariant Diffusion\n",
        "## De Novo Drug Design Training\n",
        "\n",
        "**All processing on LOCAL Colab disk (fast!)**\n",
        "\n",
        "**Checkpoints saved to Drive (persistent)**"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": ["## Cell 1: Setup"],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import torch\n",
        "print(f'GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"None!\"}')\n",
        "\n",
        "%pip install -q torch-geometric rdkit scipy numpy pyyaml tqdm scikit-learn\n",
        "print('âœ… Setup complete')"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": ["## Cell 2: Clone Repo"],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "REPO = '/content/drive/MyDrive/geom_diffusion'\n",
        "if not os.path.exists(REPO):\n",
        "    !git clone https://github.com/Nethrananda21/geom_diffusion.git {REPO}\n",
        "%cd {REPO}\n",
        "!git pull origin master"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 3: Download Pre-processed Data (Zenodo)\n",
        "\n",
        "**crossdocked_pocket10** - 1.6GB preprocessed dataset from PMDM paper"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "DATA_DIR = '/content/data'\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "\n",
        "# Download from Zenodo (verified working)\n",
        "if not os.path.exists(f'{DATA_DIR}/crossdocked_pocket10'):\n",
        "    print('ðŸ“¥ Downloading crossdocked_pocket10 from Zenodo (1.6GB)...')\n",
        "    !wget -q --show-progress -O {DATA_DIR}/crossdocked_pocket10.tar.gz \\\n",
        "        https://zenodo.org/record/10519710/files/crossdocked_pocket10.tar.gz\n",
        "    \n",
        "    print('ðŸ“¦ Extracting...')\n",
        "    !tar -xzf {DATA_DIR}/crossdocked_pocket10.tar.gz -C {DATA_DIR}/\n",
        "    !rm {DATA_DIR}/crossdocked_pocket10.tar.gz\n",
        "    print('âœ… Done!')\n",
        "else:\n",
        "    print('âœ… Data already exists')\n",
        "\n",
        "!ls -la {DATA_DIR}/"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": ["## Cell 4: Inspect Data Structure"],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "\n",
        "DATA_DIR = '/content/data/crossdocked_pocket10'\n",
        "\n",
        "# List available files\n",
        "print('ðŸ“ Files in dataset:')\n",
        "for f in os.listdir(DATA_DIR):\n",
        "    size = os.path.getsize(f'{DATA_DIR}/{f}') / 1e6\n",
        "    print(f'  {f}: {size:.1f} MB')\n",
        "\n",
        "# Check for index.pkl\n",
        "index_file = f'{DATA_DIR}/index.pkl'\n",
        "if os.path.exists(index_file):\n",
        "    with open(index_file, 'rb') as f:\n",
        "        index = pickle.load(f)\n",
        "    print(f'\\nðŸ“Š Index: {len(index)} samples')\n",
        "    print(f'   Sample keys: {list(index[0].keys()) if hasattr(index[0], \"keys\") else type(index[0])}')"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 5: Create Stratified 5K Subset\n",
        "\n",
        "Size stratification + diversity + no data leakage"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from sklearn.cluster import KMeans\n",
        "import os\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "DATA_DIR = '/content/data/crossdocked_pocket10'\n",
        "OUT_DIR = '/content/data/crossdocked'\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "# Load index\n",
        "with open(f'{DATA_DIR}/index.pkl', 'rb') as f:\n",
        "    full_data = pickle.load(f)\n",
        "print(f'Loaded {len(full_data)} samples')\n",
        "\n",
        "# Group by pocket\n",
        "pockets = defaultdict(list)\n",
        "pocket_info = {}\n",
        "\n",
        "for i, sample in enumerate(full_data):\n",
        "    if isinstance(sample, dict):\n",
        "        pocket_id = sample.get('pocket_id', sample.get('protein_filename', f'pocket_{i}'))\n",
        "        pocket_coords = np.array(sample.get('protein_pos', sample.get('pocket_coords', [])))\n",
        "    else:\n",
        "        pocket_id = str(i)\n",
        "        pocket_coords = np.zeros((1, 3))\n",
        "    \n",
        "    pockets[pocket_id].append(sample)\n",
        "    if pocket_id not in pocket_info:\n",
        "        pocket_info[pocket_id] = {\n",
        "            'size': len(pocket_coords),\n",
        "            'centroid': pocket_coords.mean(axis=0) if len(pocket_coords) > 0 else np.zeros(3)\n",
        "        }\n",
        "\n",
        "print(f'Found {len(pockets)} unique pockets')\n",
        "\n",
        "# Filter by size and ligand count\n",
        "MAX_SIZE, MIN_LIGANDS = 250, 10  # Relaxed for this dataset\n",
        "valid = [p for p, s in pockets.items() \n",
        "         if pocket_info[p]['size'] <= MAX_SIZE and len(s) >= MIN_LIGANDS]\n",
        "print(f'After filter: {len(valid)} pockets')\n",
        "\n",
        "# Stratify by size\n",
        "small = [p for p in valid if pocket_info[p]['size'] <= 100]\n",
        "medium = [p for p in valid if 100 < pocket_info[p]['size'] <= 175]\n",
        "large = [p for p in valid if 175 < pocket_info[p]['size'] <= 250]\n",
        "\n",
        "print(f'Bins: Small={len(small)}, Medium={len(medium)}, Large={len(large)}')\n",
        "\n",
        "# Select diverse pockets using K-means\n",
        "def select_diverse(plist, n):\n",
        "    if len(plist) <= n: return plist\n",
        "    centroids = np.array([pocket_info[p]['centroid'] for p in plist])\n",
        "    km = KMeans(n_clusters=min(n, len(plist)), random_state=42, n_init=10).fit(centroids)\n",
        "    return [plist[[i for i in range(len(plist)) if km.labels_[i] == c][0]] for c in range(min(n, len(plist))) if any(km.labels_ == c)]\n",
        "\n",
        "# 40 from each bin\n",
        "sel_small = select_diverse(small, min(40, len(small)))\n",
        "sel_medium = select_diverse(medium, min(40, len(medium)))\n",
        "sel_large = select_diverse(large, min(40, len(large)))\n",
        "\n",
        "# Stratified split (preserve ratios)\n",
        "np.random.shuffle(sel_small); np.random.shuffle(sel_medium); np.random.shuffle(sel_large)\n",
        "\n",
        "n_train_small = int(len(sel_small) * 0.83)\n",
        "n_train_medium = int(len(sel_medium) * 0.83)\n",
        "n_train_large = int(len(sel_large) * 0.83)\n",
        "\n",
        "train_pockets = sel_small[:n_train_small] + sel_medium[:n_train_medium] + sel_large[:n_train_large]\n",
        "val_pockets = sel_small[n_train_small:] + sel_medium[n_train_medium:] + sel_large[n_train_large:]\n",
        "\n",
        "print(f'\\nâœ… Train: {len(train_pockets)} pockets, Val: {len(val_pockets)} pockets')\n",
        "print(f'âœ… Overlap: {len(set(train_pockets) & set(val_pockets))}')\n",
        "\n",
        "# Create datasets (up to 50 ligands per pocket)\n",
        "train_samples = [s for p in train_pockets for s in pockets[p][:50]]\n",
        "val_samples = [s for p in val_pockets for s in pockets[p][:50]]\n",
        "\n",
        "print(f'ðŸ“Š Train: {len(train_samples)}, Val: {len(val_samples)}')\n",
        "\n",
        "# Save\n",
        "with open(f'{OUT_DIR}/train_data.pkl', 'wb') as f:\n",
        "    pickle.dump(train_samples, f)\n",
        "with open(f'{OUT_DIR}/val_data.pkl', 'wb') as f:\n",
        "    pickle.dump(val_samples, f)\n",
        "\n",
        "print('ðŸ’¾ Saved to /content/data/crossdocked/')"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": ["## Cell 6: Update Config"],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import yaml\n",
        "\n",
        "%cd /content/drive/MyDrive/geom_diffusion\n",
        "\n",
        "with open('configs/debug_t4.yaml', 'r') as f:\n",
        "    cfg = yaml.safe_load(f)\n",
        "\n",
        "cfg['data']['root'] = '/content/data'\n",
        "cfg['data']['train_file'] = 'crossdocked/train_data.pkl'\n",
        "cfg['data']['val_file'] = 'crossdocked/val_data.pkl'\n",
        "cfg['training']['max_epochs'] = 50\n",
        "cfg['training']['batch_size'] = 4\n",
        "cfg['hardware']['num_workers'] = 2\n",
        "\n",
        "with open('configs/debug_t4.yaml', 'w') as f:\n",
        "    yaml.dump(cfg, f)\n",
        "\n",
        "print('âœ… Config updated')"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": ["## Cell 7: Delete Cache & Train ðŸš€"],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "%cd /content/drive/MyDrive/geom_diffusion\n",
        "\n",
        "# Delete cache\n",
        "for cache in ['/content/data/cache', 'data/cache']:\n",
        "    if Path(cache).exists():\n",
        "        shutil.rmtree(cache)\n",
        "        print(f'ðŸ—‘ï¸ Deleted {cache}')\n",
        "\n",
        "# Train\n",
        "!python train.py --config configs/debug_t4.yaml --checkpoint_dir checkpoints"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": ["## Cell 8: Resume Training"],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# After disconnect: Run Cells 1, 2, 6 then this:\n",
        "# %cd /content/drive/MyDrive/geom_diffusion\n",
        "# !python train.py --config configs/debug_t4.yaml --resume checkpoints/best_model.pt"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}
