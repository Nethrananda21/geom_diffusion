{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üß¨ Geometry-Complete Equivariant Diffusion Model\n",
        "## De Novo Drug Design Training Notebook\n",
        "\n",
        "**Requirements:** GPU Runtime (T4), ~55GB disk space"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": ["## Cell 1: Check GPU & Disk Space"],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import shutil\n",
        "\n",
        "# GPU check\n",
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"‚úÖ GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"   VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "else:\n",
        "    print(\"‚ùå No GPU! Runtime > Change runtime type > GPU\")\n",
        "\n",
        "# Disk check\n",
        "total, used, free = shutil.disk_usage('/')\n",
        "print(f\"\\nüíæ Disk: {free / 1e9:.1f} GB free of {total / 1e9:.1f} GB\")\n",
        "if free / 1e9 < 55:\n",
        "    print(\"‚ö†Ô∏è Less than 55GB free - may not fit full dataset\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": ["## Cell 2: Install Dependencies"],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q torch-geometric rdkit scipy numpy pyyaml tqdm wandb\n",
        "from rdkit import Chem\n",
        "print(\"‚úÖ All dependencies installed\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": ["## Cell 3: Clone Repository"],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Nethrananda21/geom_diffusion.git\n",
        "%cd geom_diffusion\n",
        "!git pull origin master"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 4: Download & Extract Dataset (Stream Mode - Saves 50GB!)\n",
        "\n",
        "‚ö†Ô∏è This downloads and extracts simultaneously, never storing the full 50GB archive.\n",
        "\n",
        "**Skip this cell to use synthetic data for quick testing.**"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Create data directory\n",
        "os.makedirs('data', exist_ok=True)\n",
        "\n",
        "print(\"üì• Downloading and extracting CrossDocked2020 (streaming mode)...\")\n",
        "print(\"   This takes 30-60 minutes. Do NOT close the browser.\")\n",
        "\n",
        "# Stream download + extract (no intermediate .tgz file saved)\n",
        "!wget -O - --no-check-certificate http://bits.csb.pitt.edu/files/crossdock2020/CrossDocked2020_v1.3.tgz 2>/dev/null | tar -xzf - -C ./data/\n",
        "\n",
        "# Verify extraction\n",
        "if os.path.exists('data/CrossDocked2020'):\n",
        "    print(\"\\n‚úÖ Dataset extracted successfully to data/CrossDocked2020/\")\n",
        "    !du -sh data/CrossDocked2020\n",
        "else:\n",
        "    # Check if extracted with different folder name\n",
        "    !ls -la data/\n",
        "    print(\"\\n‚ö†Ô∏è Check folder name above - may need to rename\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": ["## Cell 5: Preprocess Dataset (Creates .pkl files)"],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the extracted folder (might have different name)\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "data_dir = None\n",
        "for folder in ['CrossDocked2020', 'crossdocked2020', 'CrossDocked2020_v1.3']:\n",
        "    if Path(f'data/{folder}').exists():\n",
        "        data_dir = f'data/{folder}'\n",
        "        break\n",
        "\n",
        "if data_dir:\n",
        "    print(f\"üìÇ Found dataset at: {data_dir}\")\n",
        "    print(\"‚è≥ Preprocessing (this takes 10-20 minutes)...\")\n",
        "    !python preprocess_crossdocked.py \\\n",
        "        --data_dir {data_dir} \\\n",
        "        --output_dir ./data/crossdocked \\\n",
        "        --config configs/debug_t4.yaml\n",
        "    print(\"\\n‚úÖ Preprocessing complete\")\n",
        "    !ls -la data/crossdocked/\n",
        "else:\n",
        "    print(\"‚ùå Dataset folder not found. Check Cell 4 output.\")\n",
        "    !ls -la data/"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": ["## Cell 6: Delete Old Cache (CRITICAL!)"],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "# Delete synthetic cache\n",
        "cache_dir = Path('data/cache')\n",
        "if cache_dir.exists():\n",
        "    shutil.rmtree(cache_dir)\n",
        "    print(\"üóëÔ∏è Deleted old cache\")\n",
        "else:\n",
        "    print(\"‚ÑπÔ∏è No cache to delete\")\n",
        "\n",
        "# Verify real data\n",
        "train_pkl = Path('data/crossdocked/train_data.pkl')\n",
        "val_pkl = Path('data/crossdocked/val_data.pkl')\n",
        "\n",
        "if train_pkl.exists() and val_pkl.exists():\n",
        "    print(f\"\\n‚úÖ Ready to train on REAL data:\")\n",
        "    print(f\"   - {train_pkl} ({train_pkl.stat().st_size / 1e6:.1f} MB)\")\n",
        "    print(f\"   - {val_pkl} ({val_pkl.stat().st_size / 1e6:.1f} MB)\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è Real data NOT found - will use SYNTHETIC\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": ["## Cell 7: Configure Training"],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import yaml\n",
        "\n",
        "with open('configs/debug_t4.yaml', 'r') as f:\n",
        "    config = yaml.safe_load(f)\n",
        "\n",
        "# Colab optimizations\n",
        "config['training']['max_epochs'] = 10  # Quick test\n",
        "config['hardware']['num_workers'] = 2  # Colab CPU limit\n",
        "\n",
        "with open('configs/debug_t4.yaml', 'w') as f:\n",
        "    yaml.dump(config, f)\n",
        "\n",
        "print(f\"‚úÖ Config: batch={config['training']['batch_size']}, epochs={config['training']['max_epochs']}\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": ["## Cell 8: Start Training üöÄ"],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Watch logs - should say \"Loading preprocessed data\" NOT \"SYNTHETIC\"\n",
        "!python train.py --config configs/debug_t4.yaml --checkpoint_dir ./checkpoints"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": ["## Cell 9: Resume Training (If Interrupted)"],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Uncomment to resume\n",
        "# !python train.py --config configs/debug_t4.yaml --resume ./checkpoints/best_model.pt"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": ["## Cell 10: Download Trained Model"],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "from pathlib import Path\n",
        "\n",
        "ckpt = Path('checkpoints/best_model.pt')\n",
        "if ckpt.exists():\n",
        "    files.download(str(ckpt))\n",
        "    print(\"‚úÖ Model downloaded\")\n",
        "else:\n",
        "    print(\"‚ùå No checkpoint yet\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}
