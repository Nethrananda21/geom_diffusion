{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ§¬ Geometry-Complete Equivariant Diffusion\n",
        "## De Novo Drug Design Training\n",
        "\n",
        "**Data**: Pitt.edu direct wget"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": ["## Cell 1: Setup"],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import torch\n",
        "print(f'GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"None!\"}')\n",
        "\n",
        "%pip install -q torch-geometric rdkit scipy numpy pyyaml tqdm scikit-learn\n",
        "print('âœ… Setup complete')"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": ["## Cell 2: Clone Repo"],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "REPO = '/content/drive/MyDrive/geom_diffusion'\n",
        "if not os.path.exists(REPO):\n",
        "    !git clone https://github.com/Nethrananda21/geom_diffusion.git {REPO}\n",
        "%cd {REPO}\n",
        "!git pull origin master"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": ["## Cell 3: Download CrossDocked"],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "DATA_DIR = '/content/data/raw'\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "\n",
        "existing = [d for d in os.listdir(DATA_DIR) if os.path.isdir(f'{DATA_DIR}/{d}')]\n",
        "\n",
        "if len(existing) < 10:\n",
        "    print('ðŸ“¥ Downloading downsampled CrossDocked2020 (~5GB)...')\n",
        "    !wget -q --show-progress -O /content/crossdocked.tgz \\\n",
        "        https://bits.csb.pitt.edu/files/crossdock2020/downsampled_CrossDocked2020_v1.3.tgz\n",
        "    \n",
        "    print('\\nðŸ“¦ Extracting...')\n",
        "    !tar -xzf /content/crossdocked.tgz -C {DATA_DIR}/\n",
        "    !rm /content/crossdocked.tgz\n",
        "    print('âœ… Done!')\n",
        "else:\n",
        "    print(f'âœ… Data exists: {len(existing)} folders')\n",
        "\n",
        "folders = [d for d in os.listdir(DATA_DIR) if os.path.isdir(f'{DATA_DIR}/{d}')]\n",
        "print(f'ðŸ“ Total pocket folders: {len(folders)}')"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": ["## Cell 4: Analyze Size Distribution"],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "\n",
        "DATA_DIR = '/content/data/raw'\n",
        "pocket_dirs = [Path(DATA_DIR) / d for d in os.listdir(DATA_DIR) if os.path.isdir(f'{DATA_DIR}/{d}')]\n",
        "\n",
        "# Sample 500 pockets to understand distribution\n",
        "sizes = []\n",
        "for pocket_dir in tqdm(pocket_dirs[:500], desc='Analyzing'):\n",
        "    all_pdb = list(pocket_dir.glob('*.pdb'))\n",
        "    if all_pdb:\n",
        "        try:\n",
        "            with open(all_pdb[0], 'r') as f:\n",
        "                atom_count = sum(1 for line in f if line.startswith('ATOM'))\n",
        "            sizes.append(atom_count)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "sizes = np.array(sizes)\n",
        "print(f'\\nðŸ“Š Pocket size distribution (atoms):')\n",
        "print(f'   Min: {sizes.min()}, Max: {sizes.max()}')\n",
        "print(f'   Mean: {sizes.mean():.0f}, Median: {np.median(sizes):.0f}')\n",
        "print(f'   Percentiles: 25%={np.percentile(sizes,25):.0f}, 50%={np.percentile(sizes,50):.0f}, 75%={np.percentile(sizes,75):.0f}')\n",
        "\n",
        "# Suggest max size\n",
        "p75 = np.percentile(sizes, 75)\n",
        "print(f'\\nðŸ’¡ Recommended max size: {int(p75)} atoms (75th percentile)')"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": ["## Cell 5: Create 5K Subset (Adjusted Filters)"],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "DATA_DIR = '/content/data/raw'\n",
        "OUT_DIR = '/content/data/crossdocked'\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "# ADJUSTED FILTERS for this dataset\n",
        "MAX_SIZE = 800  # Increased! Most pockets are larger\n",
        "MIN_LIGANDS = 3  # Relaxed - avg is 7\n",
        "\n",
        "print(f'âš™ï¸ Filters: max_size={MAX_SIZE}, min_ligands={MIN_LIGANDS}')\n",
        "\n",
        "pocket_dirs = [Path(DATA_DIR) / d for d in os.listdir(DATA_DIR) if os.path.isdir(f'{DATA_DIR}/{d}')]\n",
        "print(f'Found {len(pocket_dirs)} pocket directories')\n",
        "\n",
        "pockets = defaultdict(list)\n",
        "pocket_info = {}\n",
        "\n",
        "for pocket_dir in tqdm(pocket_dirs, desc='Processing'):\n",
        "    pocket_id = pocket_dir.name\n",
        "    all_pdb = list(pocket_dir.glob('*.pdb'))\n",
        "    all_sdf = list(pocket_dir.glob('*.sdf'))\n",
        "    \n",
        "    pocket_pdb = [p for p in all_pdb if 'rec' in p.name.lower() or 'pocket' in p.name.lower()]\n",
        "    if not pocket_pdb:\n",
        "        pocket_pdb = all_pdb[:1]\n",
        "    \n",
        "    if pocket_pdb and all_sdf:\n",
        "        try:\n",
        "            with open(pocket_pdb[0], 'r') as f:\n",
        "                atom_count = sum(1 for line in f if line.startswith('ATOM'))\n",
        "        except:\n",
        "            atom_count = 500\n",
        "        \n",
        "        pocket_info[pocket_id] = {'size': atom_count, 'pdb': str(pocket_pdb[0])}\n",
        "        \n",
        "        for lig in all_sdf:\n",
        "            pockets[pocket_id].append({\n",
        "                'pocket_pdb': str(pocket_pdb[0]),\n",
        "                'ligand_sdf': str(lig),\n",
        "                'pocket_id': pocket_id,\n",
        "                'num_atoms': atom_count\n",
        "            })\n",
        "\n",
        "print(f'\\nâœ… Processed {len(pockets)} pockets')\n",
        "\n",
        "# Apply filters\n",
        "valid = [p for p, samples in pockets.items() \n",
        "         if pocket_info[p]['size'] <= MAX_SIZE and len(samples) >= MIN_LIGANDS]\n",
        "print(f'After filter: {len(valid)} pockets')\n",
        "\n",
        "# Stratify by size (adjusted bins)\n",
        "sizes = [pocket_info[p]['size'] for p in valid]\n",
        "p33, p66 = np.percentile(sizes, [33, 66])\n",
        "print(f'Size terciles: small<{p33:.0f}, medium<{p66:.0f}, large>={p66:.0f}')\n",
        "\n",
        "small = [p for p in valid if pocket_info[p]['size'] <= p33]\n",
        "medium = [p for p in valid if p33 < pocket_info[p]['size'] <= p66]\n",
        "large = [p for p in valid if pocket_info[p]['size'] > p66]\n",
        "\n",
        "print(f'Bins: Small={len(small)}, Medium={len(medium)}, Large={len(large)}')\n",
        "\n",
        "# Select from each bin\n",
        "np.random.shuffle(small)\n",
        "np.random.shuffle(medium)\n",
        "np.random.shuffle(large)\n",
        "\n",
        "n_small = min(40, len(small))\n",
        "n_medium = min(40, len(medium))\n",
        "n_large = min(40, len(large))\n",
        "selected = small[:n_small] + medium[:n_medium] + large[:n_large]\n",
        "\n",
        "print(f'\\nSelected: {len(selected)} pockets')\n",
        "\n",
        "# Split\n",
        "np.random.shuffle(selected)\n",
        "split_idx = int(len(selected) * 0.83)\n",
        "train_pockets = selected[:split_idx]\n",
        "val_pockets = selected[split_idx:]\n",
        "\n",
        "print(f'Train: {len(train_pockets)}, Val: {len(val_pockets)}')\n",
        "\n",
        "# Create datasets\n",
        "train_samples = [s for p in train_pockets for s in pockets[p][:50]]\n",
        "val_samples = [s for p in val_pockets for s in pockets[p][:50]]\n",
        "\n",
        "print(f'ðŸ“Š Train: {len(train_samples)}, Val: {len(val_samples)}')\n",
        "\n",
        "# Save\n",
        "with open(f'{OUT_DIR}/train_data.pkl', 'wb') as f:\n",
        "    pickle.dump(train_samples, f)\n",
        "with open(f'{OUT_DIR}/val_data.pkl', 'wb') as f:\n",
        "    pickle.dump(val_samples, f)\n",
        "\n",
        "print('ðŸ’¾ Saved!')"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": ["## Cell 6: Update Config"],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import yaml\n",
        "\n",
        "%cd /content/drive/MyDrive/geom_diffusion\n",
        "\n",
        "with open('configs/debug_t4.yaml', 'r') as f:\n",
        "    cfg = yaml.safe_load(f)\n",
        "\n",
        "cfg['data']['root'] = '/content/data'\n",
        "cfg['data']['train_file'] = 'crossdocked/train_data.pkl'\n",
        "cfg['data']['val_file'] = 'crossdocked/val_data.pkl'\n",
        "cfg['training']['max_epochs'] = 50\n",
        "cfg['training']['batch_size'] = 2  # Reduced for larger pockets\n",
        "cfg['hardware']['num_workers'] = 2\n",
        "\n",
        "with open('configs/debug_t4.yaml', 'w') as f:\n",
        "    yaml.dump(cfg, f)\n",
        "\n",
        "print('âœ… Config updated (batch_size=2 for larger pockets)')"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": ["## Cell 7: Train ðŸš€"],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "%cd /content/drive/MyDrive/geom_diffusion\n",
        "\n",
        "for cache in ['/content/data/cache', 'data/cache']:\n",
        "    if Path(cache).exists():\n",
        "        shutil.rmtree(cache)\n",
        "\n",
        "!python train.py --config configs/debug_t4.yaml --checkpoint_dir checkpoints"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}
