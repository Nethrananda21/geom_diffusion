{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üß¨ Geometry-Complete Equivariant Diffusion\n",
        "## De Novo Drug Design Training\n",
        "\n",
        "**Data**: CrossDocked2020 (1000 atom limit, batch_size=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["## Cell 1: Setup"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import torch\n",
        "print(f'GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"None!\"}')\n",
        "\n",
        "%pip install -q torch-geometric rdkit scipy numpy pyyaml tqdm scikit-learn\n",
        "print('‚úÖ Setup complete')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["## Cell 2: Clone Repo"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "REPO = '/content/drive/MyDrive/geom_diffusion'\n",
        "if not os.path.exists(REPO):\n",
        "    !git clone https://github.com/Nethrananda21/geom_diffusion.git {REPO}\n",
        "%cd {REPO}\n",
        "!git pull origin master"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["## Cell 3: Extract Data from Drive Backup"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "DATA_DIR = '/content/data/raw'\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "\n",
        "existing = [d for d in os.listdir(DATA_DIR) if os.path.isdir(f'{DATA_DIR}/{d}')]\n",
        "\n",
        "if len(existing) < 10:\n",
        "    if os.path.exists('/content/drive/MyDrive/crossdocked_essential.tar.gz'):\n",
        "        print('üì¶ Extracting from Drive backup...')\n",
        "        !tar -xzf /content/drive/MyDrive/crossdocked_essential.tar.gz -C {DATA_DIR}/\n",
        "        print('‚úÖ Done!')\n",
        "    else:\n",
        "        print('‚ùå No backup found.')\n",
        "else:\n",
        "    print(f'‚úÖ Data exists: {len(existing)} folders')\n",
        "\n",
        "folders = [d for d in os.listdir(DATA_DIR) if os.path.isdir(f'{DATA_DIR}/{d}')]\n",
        "print(f'üìÅ Total pocket folders: {len(folders)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["## Cell 4: Preprocess (1000 atom limit)"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "from rdkit import Chem\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# === CONFIG ===\n",
        "MAX_POCKET_ATOMS = 1000  # Increased for this dataset\n",
        "MAX_LIGAND_ATOMS = 50\n",
        "MIN_LIGAND_ATOMS = 5\n",
        "MIN_LIGANDS_PER_POCKET = 3\n",
        "N_POCKETS_PER_BIN = 30\n",
        "\n",
        "ATOM_TYPES = ['C', 'N', 'O', 'S', 'P', 'F', 'Cl', 'Br', 'I', 'Other']\n",
        "ATOM_TO_IDX = {a: i for i, a in enumerate(ATOM_TYPES)}\n",
        "\n",
        "def parse_pdb(pdb_path):\n",
        "    coords = []\n",
        "    types = []\n",
        "    with open(pdb_path, 'r') as f:\n",
        "        for line in f:\n",
        "            if line.startswith('ATOM'):\n",
        "                x = float(line[30:38])\n",
        "                y = float(line[38:46])\n",
        "                z = float(line[46:54])\n",
        "                coords.append([x, y, z])\n",
        "                elem = line[76:78].strip() if len(line) > 77 else line[12:16].strip()[0]\n",
        "                idx = ATOM_TO_IDX.get(elem, 9)\n",
        "                one_hot = [0.0] * 10\n",
        "                one_hot[idx] = 1.0\n",
        "                types.append(one_hot)\n",
        "    return np.array(coords, dtype=np.float32), np.array(types, dtype=np.float32)\n",
        "\n",
        "def parse_sdf(sdf_path):\n",
        "    mol = Chem.SDMolSupplier(str(sdf_path), removeHs=True, sanitize=False)[0]\n",
        "    if mol is None:\n",
        "        return None, None\n",
        "    try:\n",
        "        conf = mol.GetConformer()\n",
        "    except:\n",
        "        return None, None\n",
        "    coords = []\n",
        "    types = []\n",
        "    for atom in mol.GetAtoms():\n",
        "        pos = conf.GetAtomPosition(atom.GetIdx())\n",
        "        coords.append([pos.x, pos.y, pos.z])\n",
        "        elem = atom.GetSymbol()\n",
        "        idx = ATOM_TO_IDX.get(elem, 9)\n",
        "        one_hot = [0.0] * 10\n",
        "        one_hot[idx] = 1.0\n",
        "        types.append(one_hot)\n",
        "    return np.array(coords, dtype=np.float32), np.array(types, dtype=np.float32)\n",
        "\n",
        "print(f'üî¨ Preprocessing (max pocket atoms: {MAX_POCKET_ATOMS})...')\n",
        "\n",
        "DATA_DIR = '/content/data/raw'\n",
        "OUT_DIR = '/content/data/crossdocked'\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "pocket_dirs = [Path(DATA_DIR) / d for d in os.listdir(DATA_DIR) if os.path.isdir(f'{DATA_DIR}/{d}')]\n",
        "print(f'Found {len(pocket_dirs)} pocket directories')\n",
        "\n",
        "pockets = defaultdict(list)\n",
        "pocket_info = {}\n",
        "\n",
        "for pocket_dir in tqdm(pocket_dirs, desc='Processing'):\n",
        "    pocket_id = pocket_dir.name\n",
        "    rec_pdbs = list(pocket_dir.glob('*_rec.pdb'))\n",
        "    all_sdf = list(pocket_dir.glob('*.sdf'))\n",
        "    \n",
        "    if not rec_pdbs or not all_sdf:\n",
        "        continue\n",
        "    \n",
        "    try:\n",
        "        pocket_coords, pocket_types = parse_pdb(rec_pdbs[0])\n",
        "        if len(pocket_coords) == 0 or len(pocket_coords) > MAX_POCKET_ATOMS:\n",
        "            continue\n",
        "    except:\n",
        "        continue\n",
        "    \n",
        "    pocket_info[pocket_id] = {'size': len(pocket_coords)}\n",
        "    \n",
        "    for sdf_path in all_sdf[:50]:\n",
        "        try:\n",
        "            lig_coords, lig_types = parse_sdf(sdf_path)\n",
        "            if lig_coords is None:\n",
        "                continue\n",
        "            if len(lig_coords) < MIN_LIGAND_ATOMS or len(lig_coords) > MAX_LIGAND_ATOMS:\n",
        "                continue\n",
        "            \n",
        "            pockets[pocket_id].append({\n",
        "                'pocket_id': pocket_id,\n",
        "                'ligand_id': sdf_path.stem,\n",
        "                'ligand_coords': lig_coords,\n",
        "                'ligand_types': lig_types,\n",
        "                'pocket_coords': pocket_coords,\n",
        "                'pocket_types': pocket_types\n",
        "            })\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "print(f'\\n‚úÖ Processed {len(pockets)} valid pockets (‚â§{MAX_POCKET_ATOMS} atoms)')\n",
        "\n",
        "valid = [p for p, samples in pockets.items() if len(samples) >= MIN_LIGANDS_PER_POCKET]\n",
        "print(f'After filter (lig>={MIN_LIGANDS_PER_POCKET}): {len(valid)} pockets')\n",
        "\n",
        "if len(valid) == 0:\n",
        "    print('‚ùå No valid pockets! Try increasing MAX_POCKET_ATOMS')\n",
        "else:\n",
        "    sizes = [pocket_info[p]['size'] for p in valid]\n",
        "    p33, p66 = np.percentile(sizes, [33, 66])\n",
        "    \n",
        "    small = [p for p in valid if pocket_info[p]['size'] <= p33]\n",
        "    medium = [p for p in valid if p33 < pocket_info[p]['size'] <= p66]\n",
        "    large = [p for p in valid if pocket_info[p]['size'] > p66]\n",
        "    \n",
        "    print(f'Bins: Small(<{p33:.0f})={len(small)}, Medium(<{p66:.0f})={len(medium)}, Large={len(large)}')\n",
        "    \n",
        "    np.random.shuffle(small)\n",
        "    np.random.shuffle(medium)\n",
        "    np.random.shuffle(large)\n",
        "    \n",
        "    n_per = min(N_POCKETS_PER_BIN, len(small), len(medium), len(large))\n",
        "    selected = small[:n_per] + medium[:n_per] + large[:n_per]\n",
        "    print(f'Selected: {len(selected)} pockets')\n",
        "    \n",
        "    np.random.shuffle(selected)\n",
        "    split_idx = int(len(selected) * 0.83)\n",
        "    train_pockets = selected[:split_idx]\n",
        "    val_pockets = selected[split_idx:]\n",
        "    \n",
        "    print(f'Train: {len(train_pockets)}, Val: {len(val_pockets)}')\n",
        "    \n",
        "    train_samples = [s for p in train_pockets for s in pockets[p]]\n",
        "    val_samples = [s for p in val_pockets for s in pockets[p]]\n",
        "    \n",
        "    print(f'\\nüìä Train: {len(train_samples)}, Val: {len(val_samples)}')\n",
        "    \n",
        "    with open(f'{OUT_DIR}/train_data.pkl', 'wb') as f:\n",
        "        pickle.dump(train_samples, f)\n",
        "    with open(f'{OUT_DIR}/val_data.pkl', 'wb') as f:\n",
        "        pickle.dump(val_samples, f)\n",
        "    \n",
        "    print('üíæ Saved!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["## Cell 5: Update Config (1000 atoms, batch_size=1)"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import yaml\n",
        "\n",
        "%cd /content/drive/MyDrive/geom_diffusion\n",
        "\n",
        "with open('configs/debug_t4.yaml', 'r') as f:\n",
        "    cfg = yaml.safe_load(f)\n",
        "\n",
        "cfg['data']['root'] = '/content/data/crossdocked'\n",
        "cfg['data']['filters']['pocket_atoms_max'] = 1000  # Match preprocessing\n",
        "cfg['training']['batch_size'] = 1  # Required for large pockets\n",
        "cfg['training']['gradient_accumulation_steps'] = 8  # Effective batch=8\n",
        "cfg['training']['max_epochs'] = 50\n",
        "cfg['hardware']['num_workers'] = 2\n",
        "\n",
        "with open('configs/debug_t4.yaml', 'w') as f:\n",
        "    yaml.dump(cfg, f)\n",
        "\n",
        "print('‚úÖ Config updated (1000 atoms, batch_size=1)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["## Cell 6: Train üöÄ"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "%cd /content/drive/MyDrive/geom_diffusion\n",
        "\n",
        "# Clear cache\n",
        "for cache in ['/content/data/cache', 'data/cache']:\n",
        "    if Path(cache).exists():\n",
        "        shutil.rmtree(cache)\n",
        "        print(f'üóëÔ∏è Deleted {cache}')\n",
        "\n",
        "!python train.py --config configs/debug_t4.yaml --checkpoint_dir checkpoints"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["## Cell 7: Resume Training"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Uncomment to resume:\n",
        "# %cd /content/drive/MyDrive/geom_diffusion\n",
        "# !python train.py --config configs/debug_t4.yaml --resume checkpoints/best_model.pt"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {"gpuType": "T4", "provenance": []},
    "kernelspec": {"display_name": "Python 3", "name": "python3"}
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
