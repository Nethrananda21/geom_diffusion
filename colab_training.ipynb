{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ§¬ Geometry-Complete Equivariant Diffusion\n",
        "## De Novo Drug Design Training\n",
        "\n",
        "**Selection Strategy**:\n",
        "- Size stratification (small/medium/large pockets)\n",
        "- Centroid clustering for diversity\n",
        "- Pocket-level split (no leakage)\n",
        "\n",
        "**Result**: 100 train pockets Ã— 50 ligands = 5,000 pairs"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": ["## Cell 1: Setup"],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import torch\n",
        "print(f'GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"None!\"}')\n",
        "\n",
        "!pip install -q torch-geometric rdkit scipy numpy pyyaml tqdm gdown scikit-learn\n",
        "print('âœ… Setup complete')"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": ["## Cell 2: Clone Repository"],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "REPO = '/content/drive/MyDrive/geom_diffusion'\n",
        "if not os.path.exists(REPO):\n",
        "    !git clone https://github.com/Nethrananda21/geom_diffusion.git {REPO}\n",
        "%cd {REPO}\n",
        "!git pull origin master"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": ["## Cell 3: Download Pre-processed Data"],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gdown\n",
        "\n",
        "DATA_DIR = '/content/data/crossdocked'\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "\n",
        "files = {\n",
        "    'train_data.pkl': '1vJyxCIqCYwP3qj4THMofdSd1rZDEQpPG',\n",
        "    'val_data.pkl': '1FpVNcdj0R5YOsaLQm6T4D5QOKZGI4Xc5'\n",
        "}\n",
        "\n",
        "for fname, fid in files.items():\n",
        "    path = f'{DATA_DIR}/{fname}'\n",
        "    if not os.path.exists(path):\n",
        "        print(f'ðŸ“¥ Downloading {fname}...')\n",
        "        gdown.download(id=fid, output=path, quiet=False)\n",
        "    else:\n",
        "        print(f'âœ… {fname} exists')"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 4: Intelligent Pocket Selection (Option B)\n",
        "\n",
        "**Strategy**:\n",
        "1. Filter pockets by size (â‰¤250 atoms for T4)\n",
        "2. Stratify: 40 small + 40 medium + 40 large = 120 pockets\n",
        "3. Use centroid clustering for diversity within each size bin\n",
        "4. Ensure â‰¥50 ligands per pocket\n",
        "5. Family-level split (train 100, val 20)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# Load data\n",
        "with open(f'{DATA_DIR}/train_data.pkl', 'rb') as f:\n",
        "    full_data = pickle.load(f)\n",
        "print(f'Loaded {len(full_data)} samples')\n",
        "\n",
        "# Inspect first sample structure\n",
        "sample = full_data[0]\n",
        "print(f'\\nSample keys: {sample.keys() if hasattr(sample, \"keys\") else type(sample)}')\n",
        "\n",
        "# Group by pocket\n",
        "pockets = defaultdict(list)\n",
        "pocket_info = {}  # Store metadata\n",
        "\n",
        "for i, sample in enumerate(full_data):\n",
        "    # Extract pocket identifier\n",
        "    if hasattr(sample, 'keys'):\n",
        "        pocket_id = sample.get('pocket_id', sample.get('receptor', f'pocket_{i}'))\n",
        "        pocket_coords = np.array(sample.get('pocket_coords', sample.get('pocket_pos', [])))\n",
        "    else:\n",
        "        # torch_geometric Data object\n",
        "        pocket_id = getattr(sample, 'pocket_id', f'pocket_{i}')\n",
        "        pocket_coords = sample.pocket_pos.numpy() if hasattr(sample, 'pocket_pos') else np.zeros((1,3))\n",
        "    \n",
        "    pockets[pocket_id].append(sample)\n",
        "    \n",
        "    # Store pocket info (first occurrence)\n",
        "    if pocket_id not in pocket_info:\n",
        "        pocket_info[pocket_id] = {\n",
        "            'size': len(pocket_coords),\n",
        "            'centroid': pocket_coords.mean(axis=0) if len(pocket_coords) > 0 else np.zeros(3)\n",
        "        }\n",
        "\n",
        "print(f'Found {len(pockets)} unique pockets')\n",
        "\n",
        "# === FILTER 1: Size (Hard T4 constraint) ===\n",
        "MAX_POCKET_ATOMS = 250\n",
        "MIN_LIGANDS = 50\n",
        "\n",
        "valid_pockets = [\n",
        "    pid for pid, samples in pockets.items()\n",
        "    if pocket_info[pid]['size'] <= MAX_POCKET_ATOMS \n",
        "    and len(samples) >= MIN_LIGANDS\n",
        "]\n",
        "print(f'After size/ligand filter: {len(valid_pockets)} pockets')\n",
        "\n",
        "# === STRATIFY by pocket size ===\n",
        "small = [p for p in valid_pockets if pocket_info[p]['size'] <= 150]\n",
        "medium = [p for p in valid_pockets if 150 < pocket_info[p]['size'] <= 200]\n",
        "large = [p for p in valid_pockets if 200 < pocket_info[p]['size'] <= 250]\n",
        "\n",
        "print(f'\\nSize distribution:')\n",
        "print(f'  Small (â‰¤150): {len(small)}')\n",
        "print(f'  Medium (151-200): {len(medium)}')\n",
        "print(f'  Large (201-250): {len(large)}')\n",
        "\n",
        "# === SELECT with diversity (centroid clustering) ===\n",
        "def select_diverse(pocket_list, n_select):\n",
        "    \"\"\"Select diverse pockets using centroid clustering\"\"\"\n",
        "    if len(pocket_list) <= n_select:\n",
        "        return pocket_list\n",
        "    \n",
        "    # Get centroids\n",
        "    centroids = np.array([pocket_info[p]['centroid'] for p in pocket_list])\n",
        "    \n",
        "    # K-means clustering\n",
        "    kmeans = KMeans(n_clusters=n_select, random_state=42, n_init=10)\n",
        "    kmeans.fit(centroids)\n",
        "    \n",
        "    # Select one pocket per cluster (closest to centroid)\n",
        "    selected = []\n",
        "    for cluster_id in range(n_select):\n",
        "        cluster_mask = kmeans.labels_ == cluster_id\n",
        "        cluster_pockets = [pocket_list[i] for i in range(len(pocket_list)) if cluster_mask[i]]\n",
        "        if cluster_pockets:\n",
        "            selected.append(cluster_pockets[0])\n",
        "    \n",
        "    return selected\n",
        "\n",
        "# Select 40 from each size bin (total 120)\n",
        "n_per_bin = 40\n",
        "selected_small = select_diverse(small, min(n_per_bin, len(small)))\n",
        "selected_medium = select_diverse(medium, min(n_per_bin, len(medium)))\n",
        "selected_large = select_diverse(large, min(n_per_bin, len(large)))\n",
        "\n",
        "all_selected = selected_small + selected_medium + selected_large\n",
        "print(f'\\nSelected: {len(selected_small)} small + {len(selected_medium)} medium + {len(selected_large)} large = {len(all_selected)} pockets')\n",
        "\n",
        "# === SPLIT: Train 100, Val 20 (pocket-level, no overlap) ===\n",
        "np.random.shuffle(all_selected)\n",
        "train_pockets = all_selected[:100]\n",
        "val_pockets = all_selected[100:120]\n",
        "\n",
        "print(f'\\nâœ… Train: {len(train_pockets)} pockets')\n",
        "print(f'âœ… Val: {len(val_pockets)} pockets')\n",
        "print(f'âœ… Overlap: {len(set(train_pockets) & set(val_pockets))} (must be 0!)')\n",
        "\n",
        "# === CREATE FINAL DATASETS ===\n",
        "LIGANDS_PER_POCKET = 50\n",
        "\n",
        "train_samples = []\n",
        "for pid in train_pockets:\n",
        "    ligands = pockets[pid][:LIGANDS_PER_POCKET]\n",
        "    train_samples.extend(ligands)\n",
        "\n",
        "val_samples = []\n",
        "for pid in val_pockets:\n",
        "    ligands = pockets[pid][:LIGANDS_PER_POCKET]\n",
        "    val_samples.extend(ligands)\n",
        "\n",
        "print(f'\\nðŸ“Š Final Dataset:')\n",
        "print(f'   Train: {len(train_samples)} samples')\n",
        "print(f'   Val: {len(val_samples)} samples')\n",
        "\n",
        "# Save\n",
        "with open(f'{DATA_DIR}/train_5k.pkl', 'wb') as f:\n",
        "    pickle.dump(train_samples, f)\n",
        "with open(f'{DATA_DIR}/val_1k.pkl', 'wb') as f:\n",
        "    pickle.dump(val_samples, f)\n",
        "\n",
        "print(f'\\nðŸ’¾ Saved: train_5k.pkl, val_1k.pkl')"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": ["## Cell 5: Update Config"],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import yaml\n",
        "\n",
        "%cd /content/drive/MyDrive/geom_diffusion\n",
        "\n",
        "with open('configs/debug_t4.yaml', 'r') as f:\n",
        "    cfg = yaml.safe_load(f)\n",
        "\n",
        "cfg['data']['root'] = '/content/data'\n",
        "cfg['data']['train_file'] = 'crossdocked/train_5k.pkl'\n",
        "cfg['data']['val_file'] = 'crossdocked/val_1k.pkl'\n",
        "cfg['training']['max_epochs'] = 50\n",
        "cfg['hardware']['num_workers'] = 2\n",
        "\n",
        "with open('configs/debug_t4.yaml', 'w') as f:\n",
        "    yaml.dump(cfg, f)\n",
        "\n",
        "print('âœ… Config updated')"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": ["## Cell 6: Delete Cache"],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "for cache in ['/content/data/cache', 'data/cache']:\n",
        "    if Path(cache).exists():\n",
        "        shutil.rmtree(cache)\n",
        "        print(f'ðŸ—‘ï¸ Deleted {cache}')"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": ["## Cell 7: Train ðŸš€"],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/geom_diffusion\n",
        "!python train.py --config configs/debug_t4.yaml --checkpoint_dir checkpoints"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": ["## Cell 8: Resume Training"],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Run Cells 1, 2, 5, 6 first, then:\n",
        "# %cd /content/drive/MyDrive/geom_diffusion\n",
        "# !python train.py --config configs/debug_t4.yaml --resume checkpoints/best_model.pt"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}
